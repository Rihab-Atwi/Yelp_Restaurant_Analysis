{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Purpose\n",
    "\n",
    "This Jupyter Notebook is created with the following objectives:\n",
    "\n",
    "## Objective 1: Filtering Non-English Reviews\n",
    "\n",
    "The first goal of this notebook is to filter out non-English reviews from a dataset. Non-English reviews can introduce noise when performing text analysis, and it's important to focus on English-language data for this analysis.\n",
    "\n",
    "## Objective 2: Applying Topic Modeling\n",
    "\n",
    "The second objective of this notebook is to apply topic modeling techniques to the filtered English-language reviews. Topic modeling helps in uncovering hidden themes or topics within textual data, which can be valuable for various applications such as sentiment analysis, content categorization, and understanding customer feedback.\n",
    "\n",
    "## Objective 3: Sentiment Analysis\n",
    "\n",
    "The third objective is to perform sentiment analysis on the English-language reviews. Sentiment analysis helps in understanding the emotional tone and sentiment expressed by customers in their reviews. This analysis will categorize reviews into positive, negative, or neutral sentiments, providing valuable insights into customer satisfaction and opinions about the restaurants.\n",
    "\n",
    "By the end of this notebook, we aim to have a clean dataset of English reviews, a set of identified topics, and sentiment scores that can be used for further analysis and insights.\n",
    "\n",
    "Let's proceed with the tasks to achieve these objectives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The main reason I am performing these two steps here and not in the ETL process is the time it takes for each of them.\n",
    "(Filtering base on language and topic modeling and sentiment analysis take more than 2 and half hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_review = pd.read_csv('C:\\\\Users\\\\User\\\\Desktop\\\\Final Project\\\\CSV_files\\\\mixed_review.csv')\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "df_review = df_review.loc[df_review['text'].apply(is_english)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346077"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review.to_csv('english_review.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective 2: Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\User\\\\Desktop\\\\Final Project\\\\english_review.csv')\n",
    "\n",
    "# Text Preprocessing\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Create an empty list to store the topics for each review\n",
    "all_topics = []\n",
    "\n",
    "for review in df['text']:\n",
    "    # Tokenize and preprocess the review text\n",
    "    tokenized_review = word_tokenize(review.lower())\n",
    "    filtered_review = [word for word in tokenized_review if word not in stop_words and word.isalpha()]\n",
    "\n",
    "    # Topic Modeling using Latent Dirichlet Allocation\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # The input for fit_transform should be a list of strings, not a list of lists\n",
    "    tfidf_review = tfidf_vectorizer.fit_transform([\" \".join(filtered_review)])\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=2, random_state=42)  # You can adjust the number of topics\n",
    "    lda.fit(tfidf_review)\n",
    "\n",
    "    # Get the top words for the topic\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    topic = lda.components_[0] if lda.components_[0].sum() > lda.components_[1].sum() else lda.components_[1]\n",
    "    top_words_idx = topic.argsort()[-5:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "\n",
    "    # Append the top words to the 'all_topics' list\n",
    "    all_topics.append(\", \".join(top_words))\n",
    "\n",
    "# Add a new column 'topic' to the DataFrame and store the topics\n",
    "df['topic'] = all_topics\n",
    "\n",
    "# Save the DataFrame to a new CSV file with the added 'topic' column\n",
    "df.to_csv('english_topic_review.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 3: Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_review = pd.read_csv('C:\\\\Users\\\\User\\\\Desktop\\\\Final Project\\\\CSV_files\\\\review_with_topic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# Initialize the sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# List to store sentiment results\n",
    "sentiments = []\n",
    "\n",
    "# Iterate through the 'text' column of the DataFrame and calculate sentiment scores\n",
    "for text in df_review['text']:\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    compound_score = sentiment['compound']\n",
    "    if compound_score >= 0.05:\n",
    "        sentiment_label = 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        sentiment_label = 'Negative'\n",
    "    else:\n",
    "        sentiment_label = 'Neutral'\n",
    "    sentiments.append({'Sentiment': sentiment_label, 'Score': compound_score})\n",
    "\n",
    "# Convert the list of sentiment results into a DataFrame\n",
    "sentiments_df = pd.DataFrame(sentiments)\n",
    "\n",
    "# Add the 'Sentiment' and 'Score' columns to the original DataFrame\n",
    "df_review['Sentiment'] = sentiments_df['Sentiment']\n",
    "df_review['Score'] = sentiments_df['Score']\n",
    "\n",
    "# Now, df_review contains the 'Sentiment' and 'Score' columns\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_review.to_csv('review.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se_etl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
