{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Purpose\n",
    "\n",
    "This Jupyter Notebook is created with the following objectives:\n",
    "\n",
    "## Objective 1: Filtering Non-English Reviews\n",
    "\n",
    "The first goal of this notebook is to filter out non-English reviews from a dataset. Non-English reviews can introduce noise when performing text analysis, and it's important to focus on English-language data for this analysis.\n",
    "\n",
    "## Objective 2: Applying Topic Modeling\n",
    "\n",
    "The second objective of this notebook is to apply topic modeling techniques to the filtered English-language reviews. Topic modeling helps in uncovering hidden themes or topics within textual data, which can be valuable for various applications such as sentiment analysis, content categorization, and understanding customer feedback.\n",
    "\n",
    "By the end of this notebook, we aim to have a clean dataset of English reviews, and a set of identified topics can be used for further analysis and insights.\n",
    "\n",
    "Let's proceed with the tasks to achieve these objectives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The main reason I am performing these two steps here and not in the ETL process is the time it takes for each of them.\n",
    "(Filtering base on language and topic modeling and sentiment analysis take more than 2 and half hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_review = pd.read_csv('C:\\\\Users\\\\User\\\\Desktop\\\\Final Project\\\\CSV_files\\\\mixed_review.csv')\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "df_review = df_review.loc[df_review['text'].apply(is_english)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review.to_csv('english_review.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective 2: Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\User\\\\Desktop\\\\Final Project\\\\english_review.csv')\n",
    "\n",
    "# Text Preprocessing\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "all_topics = []\n",
    "\n",
    "for review in df['text']:\n",
    "    tokenized_review = word_tokenize(review.lower())\n",
    "    filtered_review = [word for word in tokenized_review if word not in stop_words and word.isalpha()]\n",
    "\n",
    "    # Topic Modeling using Latent Dirichlet Allocation\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    tfidf_review = tfidf_vectorizer.fit_transform([\" \".join(filtered_review)])\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=2, random_state=42)  \n",
    "    lda.fit(tfidf_review)\n",
    "\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    topic = lda.components_[0] if lda.components_[0].sum() > lda.components_[1].sum() else lda.components_[1]\n",
    "    top_words_idx = topic.argsort()[-5:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "\n",
    "    all_topics.append(\", \".join(top_words))\n",
    "\n",
    "df['topic'] = all_topics\n",
    "\n",
    "# Save the DataFrame to a new CSV file with the added 'topic' column\n",
    "df.to_csv('english_topic_review.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se_etl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
